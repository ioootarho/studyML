---
layout: default
title: 02, モデルとは何か
nav_order: 3
math: mathjax3
---

# モデルとは何か
{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
1. TOC
{:toc}
</details>

---


## モデルの定義

機械学習の文脈で頻出するモデルという単語は「数理モデル」の意味で使用されている。  
江崎 (2020) によれば、数理モデルとは次のように定義される。

> 数理モデルとは、数学的な手段を用いて記述された、対象のデータ生成ルールを模擬したもの

## 主なモデルの種類

江崎 (2020) によれば、数理モデルはその用途から2種類に分けることができる。  

1. 理解指向型モデリング  
    データがどういうメカニズムで生成されているのかを理解することが目的
2. 応用指向型モデリング  
    手元にあるデータをもとに、未知のデータに対して予測・制御を行なったり、新しいデータを生成して利用することが目的

代表的な数理モデルには以下のようなものがある。

- 線形モデル
    - 線形モデルとは、変数同士の関係を線形結合（足し算、引き算、定数倍）で表したもの
    - 例えば変数\\(x, y\\)とパラメータ\\(a, b\\)を用いた次の式
\\[
y = ax + b
\\]
    - この式において、$x$を説明変数もしくは独立変数といい、$y$を被説明変数もしくは目的変数という
    - 特に機械学習の文脈では説明変数のことを特徴量、被説明変数のことをターゲットと呼ぶ
- 微分方程式
    - 微分の復習
      - 微分とは変数$x$が変化したときの関数$f(x)$の変化率
\\[
\frac{d}{dx}f(x) = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h}
\\]
    - 微分方程式とは、ある変数の変化に伴って見たい変数がどう変化するかを表したもの
    - 例えばマルサスの成長モデル（人口論）
        - \\(N\\)は人口を表す変数、\\(t\\)は時間を表す変数、\\(r\\)は定数とする
        - 時の経過に伴い人口がどう増減するかを表したモデル  
\\[
\frac{dN}{dt} = rN
\\]
        - 人口が多ければその分だけ生まれてくる子供も多いはず  
            &rarr; 人口の変化率はその時点での人口に比例するはず
- 統計モデル
    - 統計モデルとは、確率変数を用いて、得られたサンプルデータからその生成過程を表したもの
    - 例えば変数$x, y$とパラメータ$a, b$に加え、確率変数$\varepsilon$を用いた次のモデル    
\\[
\begin{align}
y = ax + b + \varepsilon \cr
\varepsilon \sim N(0, \sigma^2)
\end{align}
\\]
    - ただし、$\varepsilon \sim N(0, \sigma^2)$は確率変数$\varepsilon$が平均$0$ 分散$\sigma^2$の正規分布に従うことを表す  
    - 確率変数$\varepsilon$は誤差項と呼ばれるが、その意味するところは「予測値と実測値の差」ではなく「変数$x$で説明できないその他の要因」
- Box-Jenkins法
    - Box-Jenkins法とは、時系列分析における古典的アプローチを体系化したもの  
    - 所謂ARIMAモデルやその発展モデル

以上の数理モデルは、データの生成過程を理解したいシーンでよく登場するので「理解指向型モデリング」と言える。  
ただし、これらのモデルを予測に用いても何ら問題はなく、「理解指向型モデリングでは予測ができない、予測しても精度が悪い」と断言するのは誤り。

- 機械学習モデル
    - 機械学習モデルとは、機械学習アルゴリズムを用いた数理モデルの総称
    - 主な機械学習アルゴリズムは以下の通り
        - ディープラーニング
        - ツリー系アルゴリズム
        - ナイーブベイス
        - 遺伝的プログラミング
        - k-近傍法
        - サポートベクターマシン

ディープラーニングやツリー系アルゴリズムを筆頭に、機械学習モデルはデータから予測をしたり機械制御したり、とにかくアウトプットを生成したいシーンでよく登場するので「応用指向型モデリング」と言える。  
ただし、ベイズ推定を用いたモデル（階層ベイズや状態空間モデルなどのベイジアンモデル）は統計モデルとして扱われることが多く、明確に線引きをすることは難しい。  
（と言うより、ベイジアンモデルは機械学習モデルか統計モデルか、という線引き自体にあまり意味がない）  
さらに、説明可能なAI (Explanable AI; XAI) という研究分野は、機械学習モデルからなぜそのアウトプットが出てくるのか過程を理解できるようにすることを目指している。  
    cf. Permutation Importance, Partial Dependence, SHAP, Class Activation Mapping  
そのため、「応用指向型モデリングは完全なブラックボックス」と断言するのは誤り。  

## 一般的なモデル作成手順

ここでは線形回帰を例にモデル作成手順を説明する。  
複雑なモデルを作る場合には工程が増えることもあるが、一般的には以下の流れでモデルを作成する。

### 探索的データ解析

探索的データ解析 (Explanatory Data Analysis; EDA) とは、記述統計や可視化を通じてデータを理解しようとするアプローチのこと。  

- どんなデータでも必ずこれをやれば勝てる、という手法はない
- よく使われる手法は以下の通り
    - 記述統計
        - 単変量
            - 平均、分散、中央値
        - 多変量
            - クロス集計
            - 分散共分散行列（相関係数行列）
    - 可視化
        - 散布図
        - ヒストグラム
        - カーネル密度推定
        - 箱ひげ図 (Box plot)
        - 折れ線グラフ
        - ベン図
    - 次元削減
        - 主成分分析 (Principal Component Analysis; PCA)
        - 特異値分解 (Singular Value Decomposition; SVD)
    - 高次元データの可視化
        - t-SNE
        - UMAP

### 問題の定式化

- 特徴量エンジニアリング
    - 今ある特徴量から新しい特徴量を作り出すこと
        - 特徴量同士の積
        - EDAで見つけたインサイトを盛り込む
        - ドメイン知識を基にフラグを立てる
- 仮説構築
    - 特徴量とターゲットの関係について仮説を立てる
    - 例えば$n$個のサンプルについて変数\\(x, y\\)の実測値を集めたとする
    - $i$番目の実測値\\(y_i\\)を、変数\\(x_i\\)とパラメータ\\(\beta_0, \beta_1\\)、確率変数\\(\varepsilon_i\\)を用いて表す  
\\[
\begin{align}
y_i = \beta_0 + \beta_1x_i + \varepsilon_i \cr
\varepsilon_i \sim N(0, \sigma^2)
\end{align}
\\]

### 損失関数の定義

- モデルの形を決めたので、次はモデルに含まれるパラメータを求めたい
    - 歴史的理由から、モデルの形を仮説 (hypothesis) と呼ぶ
- 良いモデルであれば実測値と予測値の差が小さいはず
- 従って、次を最小にするようなパラメータ$\beta_0, \beta_1$を求めれば良い
\\[
\begin{align}
\frac{1}{n}\sum_{i=1}^{n} \{y_i - (\beta_0 + \beta_1x_i + \varepsilon_i)\} = \frac{1}{n}\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_i)
\end{align}
\\]
- ただし、誤差項\\(\varepsilon_i\\)は平均$0$なので消える
- 計算を楽にするため2乗したものを平均二乗誤差 (Mean Squared Error; MSE) と呼ぶ
\\[
\begin{align}
\frac{1}{n}\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_i)^2
\end{align}
\\]
- さらにこれの平方根を取ったものを二乗平均平方根誤差 (Root Mean Squared Error; RMSE) と呼び、MSEの代わりにRMSEを用いることもある
- このように最小化したい対象を損失関数 (Loss Function) やコスト関数 (Cost Function) と呼ぶ
- その他の主な損失関数
    - 交差エントロピー誤差 (Logarithmic Loss; Log Loss)
        - 分類問題で用いる

### 最適化手法の選択

- モデルのパラメータを求めるためには、損失関数を最小化する問題を解けば良い
- 基本的には最小化問題なので、微分して$=0$と置いて方程式を解けば良い
- 基本となる最適化手法
    - 最小二乗法 (Ordinary Least Squares)
    - 勾配降下法（最急降下法, Gradient Descent）
- その他の最適化手法
    - ニュートン法
    - LBFGS
    - 座標降下法 (Coordinate Descnt)
    - 確率的勾配降下法 (Stochastic Gradient Descnt)
    - ミニバッチ勾配降下法
    - Momentum SGD
    - Adam
- 特にディープラーニングでは色々な手法が研究されている
    - Adamが定番

### フィッティング

- フィッティングとは仮説にデータを当てはめること
- 具体的には立てた仮説に沿って、損失関数の最小化問題を選択した最適化手法で解いて、パラメータを求めること
- フィッティングのことを学習とも呼ぶ